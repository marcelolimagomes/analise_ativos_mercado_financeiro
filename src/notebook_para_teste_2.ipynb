{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ['c:\\\\Users\\\\mlimag\\\\des\\\\analise_ativos_mercado_financeiro\\\\src', 'C:\\\\Users\\\\mlimag\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python311.zip', 'C:\\\\Users\\\\mlimag\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\DLLs', 'C:\\\\Users\\\\mlimag\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib', 'C:\\\\Users\\\\mlimag\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311', 'c:\\\\Users\\\\mlimag\\\\des\\\\analise_ativos_mercado_financeiro\\\\.env', '', 'c:\\\\Users\\\\mlimag\\\\des\\\\analise_ativos_mercado_financeiro\\\\.env\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\mlimag\\\\des\\\\analise_ativos_mercado_financeiro\\\\.env\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\mlimag\\\\des\\\\analise_ativos_mercado_financeiro\\\\.env\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\mlimag\\\\des\\\\analise_ativos_mercado_financeiro\\\\.env\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if sys.path[0].endswith('/src'):\n",
    "    sys.path.insert(0, sys.path[0].removesuffix('/src'))\n",
    "print('Path:', sys.path)\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pytz\n",
    "import os\n",
    "\n",
    "from calcEMA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(load_cache = True):\n",
    "    data_file = sys.path[0] + '/src/data/ibov.csv'\n",
    "    print('Carregando Dataset...')\n",
    "    dataset = load_dataset(load_cache, data_file)     \n",
    "    print('Iniciando Calculo RSI e EMAs...')\n",
    "\n",
    "    emas_dataset = pd.DataFrame()\n",
    "    for symbol in get_tickers():\n",
    "        emas_dataset = pd.concat([emas_dataset, run_calc_emas(dataset[dataset['symbol'] == symbol], 'adj_close')])\n",
    "    \n",
    "    print('Lista ordenada por *Ações com Desconto*:', emas_dataset.index.max())\n",
    "    print_descontados(emas_dataset)\n",
    "\n",
    "\n",
    "    emas_dataset.to_csv(\n",
    "            data_file,\n",
    "            sep=';',\n",
    "            )\n",
    "    return emas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(load_cache = True, data_file = './src/data/ibov.json') -> pd.DataFrame:\n",
    "    symbols = get_tickers()    \n",
    "    if ( load_cache and os.path.exists(data_file)):\n",
    "        dataset = pd.read_json(data_file, orient='records', date_unit='s')\n",
    "        dataset.index = pd.to_datetime(dataset['date_time'])\n",
    "        dataset['date_import'] = pd.to_datetime(dataset['date_import'])\n",
    "        dataset.index.name = 'date'\n",
    "    else:\n",
    "        data = download_data('2013-01-01', symbols)\n",
    "        dataset = convert_downloaded_data(data)\n",
    "\n",
    "    print(dataset.info())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers() -> list:\n",
    "    filename = sys.path[0] + '/src/data/tickers_list_to_analisys.csv'\n",
    "    print('Tickers List File:', filename)\n",
    "    tickers = pd.read_csv(filename)\n",
    "    tickers['symbol'] += '.SA'\n",
    "    return list(tickers['symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_downloaded_data(tickers_history: pd.DataFrame) -> pd.DataFrame:\n",
    "    symbols = []\n",
    "    for symbol, _ in tickers_history.columns:\n",
    "        symbols.append(symbol)\n",
    "    # Remove duplicates\n",
    "    symbols = list(set(symbols))\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    for s in symbols:\n",
    "        aux = tickers_history[s].copy()\n",
    "        aux['symbol'] = s\n",
    "        new_df = pd.concat([new_df, aux], axis=0)\n",
    "    \n",
    "    new_df.dropna(how='any', axis=0, inplace=True) \n",
    "    new_df.rename(columns={'Adj Close': 'adj_close', 'Close': 'close', 'High': 'high',\n",
    "                  'Low': 'low', 'Open': 'open', 'Close': 'close', 'Volume': 'volume'}, inplace=True)\n",
    "    new_df.index.name = 'date'\n",
    "    new_df['date_time'] = pd.to_datetime(new_df.index)\n",
    "    new_df['date_import'] = pd.to_datetime(datetime.datetime.now(tz=pytz.UTC))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(start_date='', tickers=[]) -> pd.DataFrame:\n",
    "    if start_date == '':\n",
    "        year = datetime.datetime.today().year\n",
    "        start_date = str(year) + '-01-01'\n",
    "\n",
    "    print('Baixando dados [start_date]: ' + start_date)\n",
    "    print('Symbols: ', tickers)\n",
    "    data = yf.download(tickers, start=start_date,\n",
    "                       threads=20, group_by='ticker')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_descontados(df: pd.DataFrame):\n",
    "    filter = df[df.index == df.index.max()]\n",
    "    print(filter.sort_values(by='ema_200p_diff', ascending=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validando Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby(by='symbol').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('teste.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cRsi(df: pd.DataFrame, close_price='close', window=14):\n",
    "    '''\n",
    "    # Create two copies of the Closing price Series\n",
    "    change_up = df.copy()\n",
    "    change_down = df.copy()\n",
    "\n",
    "    # Calculate the rolling average of average up and average down\n",
    "    avg_up = change_up[close_price].rolling(14).mean()\n",
    "    avg_down = change_down[close_price].rolling(14).mean().abs()\n",
    "\n",
    "    rsi = 100 * avg_up / (avg_up + avg_down)\n",
    "    _df = df.copy()\n",
    "    _df['rsi'] = rsi\n",
    "    return _df\n",
    "    '''\n",
    "    aux = df.copy()\n",
    "    try:\n",
    "        aux['change'] = aux[close_price].diff()\n",
    "        aux['gain'] = aux.change.mask(aux.change < 0, 0.0)\n",
    "        aux['loss'] = -aux.change.mask(aux.change > 0, -0.0)\n",
    "        aux['avg_gain'] = rma(aux.gain.to_numpy(), window)\n",
    "        aux['avg_loss'] = rma(aux.loss.to_numpy(), window)\n",
    "\n",
    "        aux['rs'] = aux.avg_gain / aux.avg_loss\n",
    "        aux['rsi'] = 100 - (100 / (1 + aux.rs))\n",
    "\n",
    "\n",
    "    except Exception as error:\n",
    "        print('Erro no calculo do RSI> ', df['symbol'], ' - Data: ', df['date_time'])\n",
    "        print(error)\n",
    "        aux['rsi'] = 0.0\n",
    "    finally:\n",
    "        aux.drop(columns=['change', 'gain', 'loss', 'avg_gain', 'avg_loss', 'rs'], inplace=True, errors='ignore')\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = load_dataset(load_cache=True)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data22 = pd.DataFrame()\n",
    "print(get_tickers())\n",
    "for symbol in get_tickers():  \n",
    "  print(symbol, ':', data2[data2['symbol'] == symbol]['symbol'].count())\n",
    "  if data2[data2['symbol'] == symbol]['symbol'].count() > 14:\n",
    "    rsi_df = cRsi(data2[data2['symbol'] == symbol])\n",
    "    data22 = pd.concat([data22, rsi_df])\n",
    "    print(rsi_df.tail(1)[['symbol', 'rsi']])\n",
    "data22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__df = data2[data2['symbol'] == symbol]\n",
    "print(__df['symbol'].count())\n",
    "print(__df.tail(1))\n",
    "rsi_df = cRsi(__df)\n",
    "rsi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultimo_dia = data22.index.max()\n",
    "\n",
    "# data22[data22.index == ultimo_dia]\n",
    "data22.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = yf.download(get_tickers(), start='2013-01-01',\n",
    "                       threads=20, group_by='ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data3 = convert_downloaded_data(data3)\n",
    "new_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data3.dropna(how='any', axis=0, inplace=True)\n",
    "\n",
    "new_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data3.groupby(by='symbol').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo arquivo de cache e atualizando com dados faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 200616 entries, 2020-11-13 to 2023-06-26\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count   Dtype              \n",
      "---  ------         --------------   -----              \n",
      " 0   open           200616 non-null  float64            \n",
      " 1   high           200616 non-null  float64            \n",
      " 2   low            200616 non-null  float64            \n",
      " 3   close          200616 non-null  float64            \n",
      " 4   adj_close      200616 non-null  float64            \n",
      " 5   volume         200616 non-null  float64            \n",
      " 6   symbol         200616 non-null  object             \n",
      " 7   date_time      200616 non-null  datetime64[ns]     \n",
      " 8   date_import    200616 non-null  datetime64[ns, UTC]\n",
      " 9   ema_200p       183104 non-null  float64            \n",
      " 10  ema_200p_diff  183104 non-null  float64            \n",
      " 11  rsi            199243 non-null  float64            \n",
      "dtypes: datetime64[ns, UTC](1), datetime64[ns](1), float64(9), object(1)\n",
      "memory usage: 19.9+ MB\n"
     ]
    }
   ],
   "source": [
    "dados = pd.read_csv('.\\data\\ibov.csv', sep=';', index_col='date', parse_dates=True)\n",
    "dados['date_time'] = pd.to_datetime(dados['date_time'])\n",
    "dados['date_import'] = pd.to_datetime(dados['date_import'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-26\n",
      "2023-06-26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_date = dados.index.max()\n",
    "print(max_date.strftime('%Y-%m-%d'))\n",
    "print(datetime.datetime.now().strftime('%Y-%m-%d'))\n",
    "max_date > datetime.datetime.now()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
